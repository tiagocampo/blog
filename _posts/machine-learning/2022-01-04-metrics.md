---
layout: post
title: Model evaluation metrics
tags: machine-learning theory statistics
mathjax: true
description: In this blog post we discuss about some model evaluation metrics
---

* Do not remove this line (it will not be displayed)
{:toc}

---

The process in which machine learning algorithms _learn_ is an iterative on. The aim here is not to discuss the iterative method itself but rather the functions, often call loss or cost function, with which the iterative process of learning uses. These metrics compare the predictions with the real data, and in some sense they measure the quality of the model.

When we talk about predictive models, we are talking either about a regression model (continuous output) or a classification model (nominal or binary output). The evaluation metrics used in each of these models are different.

# Regression model

## Loss function and cost function

A function that calculates loss for 1 data point is called the loss function

$$\textrm{squared error} = \left(y_i - \hat{y}_i\right)^2$$

A function that calculates loss for the entire data being used is called the cost function

$$\textrm{mean squared error} = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2$$

The above equations are but mere examples of loss and cost functions

## Mean absolute error (MAE)

Mean absolute error, also known as L1 loss, is calculated by taking the absolute difference between the predicted values and the actual values and averaging it across the dataset. It measures only the magnitude of the errors, not their direction. The lower the MAE the higher the accuracy of a model.

$$\textrm{MAE} = \frac{1}{n} \sum_{i=1}^n \left| y_i - \hat{y}_i\right|$$


**Pros**

- Its easy to calculate
- All errors are weighted on the same scale, and provides an even measure of how well the model is performing
- It is useful if the training data has outliers as MAE does not penalize high errors caused by them

**Cons**

- It is not differentiable at zero. Many optimization algorithms tend to use differentiation to find the optimum value for parameters in the evaluation metric.

## Mean bias error (MBE)

Bias, in MBE, is the tendency of a measurement process to overestimate or underestimate the value of a parameter. A positive bias means that the error from the data is overestimated and the negative means underestimated. MBE is the mean of the difference between prediction and actual values.

$$\textrm{MBE} = \frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i\right)$$

**Pros**

- MBE is a good measure if you want to check the direction of the model and rectify it

**Cons**

- Positive and negative errors tend to cancel out each other

## Relative Absolute Error (RAE)

$$\textrm{RAE} = \frac{\sum_{i=1}^n \left| y_i - \hat{y}_i\right|}{\sum_{i=1}^n \left| y_i - \overline{y}_i\right|}$$

where

$$ \overline{y} = \frac{1}{n} \sum_{i=1}^n y_i $$

RAE ranges from 0 to 1 with a good model having RAE close to 0 and a bad on close to 1.

**Pros**

- RAE offers protection against outliers

**Cons**

- It can sometimes be undefined

## Mean squared error (MSE)

MSE is one of the most common regression loss functions and it is also known as L2 loss. Squaring the error gives higher weight to outliers, which results in a smooth gradient for small errors.

$$\textrm{MSE} = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2$$

**Pros**

- MSE values are expressed in quadratic equations, hence there is one global minima
- MSE penalizes models with huge errors

**Cons**

- It is very sensible to outliers, often readjusting the best fit to accommodate it
- Many small errors can be viewed as one large one

## Root mean squared error (RMSE)

RMSE is computed  by taking the square root of MSE. t measures the average magnitude of the errors and is concerned with the deviations from the actual value. The lower the RMSE, the better the model and its predictions

$$\textrm{RMSE} = \sqrt {\frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2 }$$

**Pros**

- It serves as heuristic for training models
- Is differentiable
- Does not penalize too much large errors

**Cons**

- It performs better if outliers are removed prior

# Classification models

In classification problems, we use two types of algorithms (dependent on the kind of output it creates):


1. Class output: Algorithms like SVM and KNN create a class output. For instance, in a binary classification problem, the outputs will be either 0 or 1. However, today we have algorithms which can convert these class outputs to probability. But these algorithms are not well accepted by the statistics community.
2. Probability output: Algorithms like Logistic Regression, Random Forest, Gradient Boosting, Adaboost etc. give probability outputs. Converting probability outputs to class output is just a matter of creating a threshold probability.

## Confusion matrix

A confusion matrix is an $$N \times N$$ matrix, where $$N$$ is the number of classes being predicted. The confusion matrix packs in itself a handful of metrics

- Accuracy: the proportion of the total number of predictions that where correct
- Positive predictive value or **Precision**: the proportion of positive cases that were correctly identified
- Negative predictive value: the proportion of negative cases that were correctly identified
 - Sensitivity or **Recal**: the proportion of actual positive cases which are correctly identified
 - **Specificity**: the proportion of actual negative cases which are correctly identified

<br/>
<div class="card center-image" style="max-width: 20rem;">
  <img src="{{site.baseurl}}/assets/images/fig1_metrics.png" class="card-img-top" alt="...">
  <div class="card-body">
    <p class="card-text">Confusion matrix. Taken from https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/</p>
  </div>
</div>
<br/>

Now let us understand this concept using hypothesis testing.

A **Hypothesis** is speculation or theory based on insufficient evidence that lends itself to further testing and experimentation. With further testing, a hypothesis can usually be proven true or false.

A **Null Hypothesis** is a hypothesis that says there is no statistical significance between the two variables in the hypothesis. It is the hypothesis that the researcher is trying to disprove.

<em>We would always reject the null hypothesis when it is false, and we would accept the null hypothesis when it is indeed true</em>

Even though hypothesis tests are meant to be reliable, there are two types of errors that can occur.

These errors are known as Type 1 and Type II errors.

For example, when examining the effectiveness of a drug, the null hypothesis would be that the drug does not affect a disease.

**Type I Error**: equivalent to False Positives(FP).

The first kind of error that is possible involves the rejection of a null hypothesis that is true.

Let’s go back to the example of a drug being used to treat a disease. If we reject the null hypothesis in this situation, then we claim that the drug does have some effect on a disease. But if the null hypothesis is true, then, in reality, the drug does not combat the disease at all. The drug is falsely claimed to have a positive effect on a disease.

**Type II Error**:- equivalent to False Negatives(FN).

The other kind of error that occurs when we accept a false null hypothesis. This sort of error is called a type II error and is also referred to as an error of the second kind.

If we think back again to the scenario in which we are testing a drug, what would a type II error look like? A type II error would occur if we accepted that the drug hs no effect on disease, but in reality, it did.

## F1 Score

F1-Score is the harmonic mean of precision and recall values for a classification problem.

$$F_1 = \left( \frac{\textrm{recall}^{-1}+\textrm{precision}^{-1}}{2} \right)$$

We use harmonic mean because it punishes extreme values more.

## Kolomogorov Smirnov chart

The Kolmogorov-Smirnov test is used to decide if a sample comes from a population with a specific distribution. 

<br/>
<div class="card center-image" style="max-width: 20rem;">
  <img src="{{site.baseurl}}/assets/images/fig2_metrics.png" class="card-img-top" alt="...">
  <div class="card-body">
    <p class="card-text">Illustration of the Kolmogorov–Smirnov statistic. The red line is a model CDF, the blue line is an empirical CDF, and the black arrow is the KS statistic.</p>
  </div>
</div>
<br/>

KS or Kolmogorov-Smirnov chart measures the performance of classification models. More accurately, K-S is a measure of the degree of separation between positive and negative distributions.

<br/>
<div class="card center-image" style="max-width: 20rem;">
  <img src="{{site.baseurl}}/assets/images/fig2_metrics.png" class="card-img-top" alt="...">
  <div class="card-body">
    <p class="card-text">The cumulative frequency for the observed and hypothesized distributions is plotted against the ordered frequencies. The vertical double arrow indicates the maximal vertical difference.</p>
  </div>
</div>
<br/>

The K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives. On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0.

In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.

##  Area Under the ROC (Receiver Operating Characteristics) Curve

Measuring the area under the ROC curve is also a very useful method for evaluating a model. By plotting the true positive rate (sensitivity) versus the false-positive rate (1 - specificity), we get the Receiver Operating Characteristic (ROC) curve.

<br/>
<div class="card center-image" style="max-width: 20rem;">
  <img src="{{site.baseurl}}/assets/images/fig4_metrics.png" class="card-img-top" alt="...">
  <div class="card-body">
    <p class="card-text">The dashed line would be random guessing (no predictive value) and is used as a baseline; anything below that is considered worse than guessing. We want to be toward the top-left corner</p>
  </div>
</div>
<br/>

Typical values the AUC are

- 90-1 = excellent (A)
- 80-.90 = good (B)
- 70-.80 = fair (C)
- 60-.70 = poor (D)
- 50-.60 = fail (F)

## Gini Coefficient

The Gini coefficient or Gini Index is a popular metric for imbalanced class values. The coefficient ranges from 0 to 1 where 0 represents perfect equality and 1 represents perfect inequality. Here, if the value of an index is higher, then the data will be more dispersed.

Gini coefficient can be computed from the area under the ROC curve using the following formula:

$$\textrm{Gini} = 2 \times AUC - 1$$

Gini above 60% is a good model.

## R-Squared/Adjusted R-Squared

We learned that when the RMSE decreases, the model’s performance will improve. But these values alone are not intuitive.

In the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how good our model is against a random model, which has an accuracy of  0.5. So the random model can be treated as a benchmark. But when we talk about the RMSE metrics, we do not have a benchmark to compare.

This is where we can use R-Squared metric. The formula for R-Squared is as follows:

$$ R^2 = 1 - \frac{\sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2}{\sum_{i=1}^n \left(y_i - \overline{y}_i\right)^2}$$ 

In other words how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.

A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared. The formula for adjusted R-Squared is given by:

$$ \overline{R^2} = 1 - \left(1-R^2\right)\left[\frac{n-1}{n-\left(k+1\right)}\right]$$

where \\(k\\) is the number of features and \\(n\\) the number of samples.

As you can see, this metric takes the number of features into account. When we add more features, the term in the denominator n-(k +1) decreases, so the whole expression increases. If R-Squared does not increase, that means the feature added isn’t valuable for our model.

# References

- [End-to-End Introduction to Evaluating Regression Models](https://www.analyticsvidhya.com/blog/2021/10/evaluation-metric-for-regression-models/){:target="_blank"}
- [11 Important Model Evaluation Metrics for Machine Learning Everyone should know](https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/){:target="_blank"}
- [Model Evaluation Metrics in Machine Learning](https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html){:target="_blank"}
- [Wikipedia Kolmogorov–Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test){:target="_blank"}