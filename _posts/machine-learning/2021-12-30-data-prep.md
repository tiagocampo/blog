---
layout: post
title: Data preprocessing for machine-learning
tags: sklearn python machine-learning theory
mathjax: false
description: In this blog post we discuss methods to treat missing data
---

* Do not remove this line (it will not be displayed)
{:toc}

---

# Data preprocessing

Data preprocessing is the process of transforming raw data into an understandable format. The quality of the data should be checked before applying any machine learning or data mining algorithms. The major tasks in data preprocessing is
    - Data cleaning
    - Data integration
    - Data reduction
    - Data transformation

In here I'll be focusing in the data cleaning part, specifically in handling the missing data either by deleting or imputation (handling the missing values with some estimation).

## Importing the dataset

First of all we should import the libraries

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

The next step is to import the dataset 

```python
dataset = pd.read_csv('Data.csv')
# matrix of features
X = dataset.iloc[:, :-1].values
# dependent variable vector
y = dataset.iloc[:, -1].values
```

Ranges in python include lowerbound and exclude upperbound, -1 specify upperbound as last column. Printing the matrix of features should result in similar output (off course each dataset should produce a different output).

```python
[['France' 44.0 72000.0]
 ['Spain' 27.0 48000.0]
 ['Germany' 30.0 54000.0]
 ['Spain' 38.0 61000.0]
 ['Germany' 40.0 nan]
 ['France' 35.0 58000.0]
 ['Spain' nan 52000.0]
 ['France' 48.0 79000.0]
 ['Germany' 50.0 83000.0]
 ['France' 37.0 67000.0]]
```

We can se that in columns 2 and 3 there are some missing values denoted by the value `nan`. 

### Deletions of missing values

Generally speaking deletion of data is not recommended but is some cases is necessary. We can simply delete the row in which a data is missing or in the case where one variable has lots of missing values we can eliminate it from the analysis. 

### Imputing of missing values

Imputation is the method of substituting missing data with some sort of _new_ data. Most common ways is to impute by mean, mode, median, linear regression, logistic regression, constants and multiple imputations.

In a given dataset we could have missing variables which are discrete, continuous or categorical.

#### Handling categorical variable

Let's say that a given categorical variable (Genre) of a dataset has some missing values. We can predict the missing values with Logistic Regression. First we split the dataset into two

```python
test = data[data['Genre'].isna()]
train = data.dropna()
```

Then we prepare the data by creating a train and test set

```python
X_train = train.drop(['Genre'], axis=1)
y_train = train['Genre']
X_test = test.drop(['Genre'], axis=1)
```

We build the logistic regression model

```python
from sklearn.linear_model import LogisticRegression
logistic = LogisticRegression(random_state = 0)
logistic.fit(X_train, y_train)
```

And predict the missing data

```python
y_pred = logistic.predict(X_test)
```

And fill-in the original dataset

```python
data.loc[data['Genre'].isnull(), 'Genre'] = y_pred
```

In some situations we could just use the mode to replace the missing data

```python
data['Genre'] = data['Genre'].fillna(data['Genre'].mode()[0])
```

#### Handling numerical variables

For numerical variables we could use mean, median or mode as follows:

```python
data['Age'] = data['Age'].fillna(data['Age'].mean()) # filling missing values by mean
data['Age'] = data['Age'].fillna(data['Age'].mode()[0]) # mode
data['Age'] = data['Age'].fillna(data['Age']).median() # median
```

Similarly to the logistic regression for categorical data we could use linear regression

```python
# Reading data
data = pd.read_csv("sales_data.csv")
# Separating missing or nan values rows
test = data[data['Age'].isna()] # Assume the Age column contains missing values.
train = data.dropna()
# Preparing Data
X_train = train.drop(['Age'], axis=1)
y_train = train['Age']
X_test = test.drop(['Age'], axis=1)
# Getting ready a model to predict missing values(Genre column)
from sklearn.linear_model import LinearRegression
linear = LinearRegression()
linear.fit(X_train, y_train)
# Predictions
y_pred = logistic.predict(X_test)
# filing the missing values with predictions
data.loc[data['Age'].isnull(), 'Age'] = y_pred
```

### Imputing with sklearn

Sklearn has a built-in method called SimpleImputer which replaces missing values with mean, median or mode. It also has a k nearest neighbor and a iterative imputation. To use them

```python
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
```

For the case of the SimpleImputer, do as follows:

```python
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 1:3])
X[:, 1:3] = imputer.transform(X[:, 1:3])
```

For the other imputers, please look at sklearn documentation.

# References

- [Data Preprocessing in Data Mining -A Hands On Guide](https://www.analyticsvidhya.com/blog/2021/08/data-preprocessing-in-data-mining-a-hands-on-guide/){:target="_blank"}
- [End-to-End Introduction to Handling Missing Values](https://www.analyticsvidhya.com/blog/2021/10/end-to-end-introduction-to-handling-missing-values/){:target="_blank"}
- [Dealing with Missing Values for Data Science Beginners](https://www.analyticsvidhya.com/blog/2021/10/guide-to-deal-with-missing-values/){:target="_blank"}
- [Missing Value Imputation](https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py){:target="_blank"}